{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "\n",
    "# Suprimir avisos específicos\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"yfinance\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_yahoo_data_history(symbol : str, period : str, interval: str, start = '1900-01-01', end = datetime.now(), prepost : bool = True):\n",
    "        '''\n",
    "        Data collection from yahoo\n",
    "\n",
    "        Parameters:\n",
    "        period : str\n",
    "            Valid periods: 1d,5d,1mo,3mo,6mo,1y,2y,5y,10y,ytd,max Either Use period parameter or use start and end\n",
    "        interval : str\n",
    "            Valid intervals: 1m,2m,5m,15m,30m,60m,90m,1h,1d,5d,1wk,1mo,3mo Intraday data cannot extend last 60 days\n",
    "        start: str\n",
    "            Download start date string (YYYY-MM-DD) or _datetime, inclusive. Default is 1900-01-01 E.g. for start=\"2020-01-01\", the first data point will be on \"2020-01-01\"\n",
    "        end: str\n",
    "            Download end date string (YYYY-MM-DD) or _datetime, exclusive. Default is now E.g. for end=\"2023-01-01\", the last data point will be on \"2022-12-31\"\n",
    "        prepost : bool\n",
    "            Include Pre and Post market data in results? Default is False\n",
    "        debug: bool\n",
    "            If passed as False, will suppress error message printing to console.\n",
    "        '''\n",
    "        yahoo_data_history = yf.Ticker(symbol).history(period=period, interval=interval, start=start, end=end, prepost=prepost)\n",
    "        return yahoo_data_history\n",
    "\n",
    "history = get_yahoo_data_history(symbol=\"MSFT\",period='1y', interval='1d', end = datetime.now(), prepost=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import talib\n",
    "import pandas as pd\n",
    "\n",
    "class CandlesPatterns:\n",
    "    \"\"\"\n",
    "    Detect all candlestick patterns using TA-Lib.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.result_candles_history_df = pd.DataFrame(columns=['Pattern', 'Signal', 'Relevance', 'Stoploss'])\n",
    "        self.result_candles_df = pd.DataFrame(columns=['Pattern', 'Signal', 'Relevance', 'Stoploss'])\n",
    "        \n",
    "        \n",
    "    def detect_pattern(self, data: pd.DataFrame, pattern_function, pattern_name: str):\n",
    "        \"\"\"\n",
    "        General method to detect a specific candlestick pattern.\n",
    "        \"\"\"\n",
    "        data_pass = data\n",
    "\n",
    "        detection = pattern_function(data_pass['Open'], data_pass['High'], data_pass['Low'], data_pass['Close'])\n",
    "\n",
    "        non_zero_detection = detection[detection != 0]\n",
    "        if not non_zero_detection.empty:\n",
    "            for date, signal in non_zero_detection.items():\n",
    "                # Cálculo do Stoploss baseado no padrão\n",
    "                if pattern_name in [\n",
    "                    \"doji\", \"dragonfly_doji\", \"gravestone_doji\", \"engulfing\",\n",
    "                    \"morning_star\", \"evening_star\", \"marubozu\", \"harami\",\n",
    "                    \"harami_cross\", \"kicking\", \"kicking_by_length\", \"tasuki_gap\",\n",
    "                    \"gap_side_by_side_white\", \"counter_attack\", \"piercing\",\n",
    "                    \"dark_cloud_cover\", \"tri_star\"\n",
    "                ]:\n",
    "                    stoploss = round(data_pass.loc[date, 'Low'],5) if signal > 0 else round(data_pass.loc[date, 'High'],5)\n",
    "                elif pattern_name in [\n",
    "                    \"morning_doji_star\", \"hammer\", \"inverted_hammer\",\n",
    "                    \"thrusting\", \"matching_low\", \"three_white_soldiers\",\n",
    "                    \"three_outside\", \"three_stars_in_south\"\n",
    "                ]:\n",
    "                    stoploss = round(data_pass.loc[date, 'Low'],5)\n",
    "                elif pattern_name in [\n",
    "                    \"evening_doji_star\", \"hanging_man\", \"shooting_star\",\n",
    "                    \"on_neck\", \"in_neck\", \"three_black_crows\",\n",
    "                    \"three_inside\", \"advance_block\", \"stalled_pattern\"\n",
    "                ]:\n",
    "                    stoploss = round(data_pass.loc[date, 'High'],5)\n",
    "                else:\n",
    "                    stoploss = None\n",
    "\n",
    "                new_entry = pd.DataFrame({\n",
    "                    'Pattern': [pattern_name],\n",
    "                    'Signal': [signal],\n",
    "                    'Relevance': ['Flat'],\n",
    "                    'Stoploss': [stoploss],\n",
    "                }, index=[date])\n",
    "\n",
    "                self.result_candles_history_df = pd.concat([self.result_candles_history_df, new_entry], ignore_index=False)\n",
    "\n",
    "                self.result_candles_df = self.result_candles_df[self.result_candles_df['Pattern'] != pattern_name]\n",
    "                self.result_candles_df = pd.concat([self.result_candles_df, new_entry], ignore_index=False)\n",
    "\n",
    "        return detection\n",
    "\n",
    "    def doji(self, data: pd.DataFrame):\n",
    "        # takeprofit = None\n",
    "        return self.detect_pattern(data, talib.CDLDOJI, \"doji\")\n",
    "\n",
    "    def dragonfly_doji(self, data: pd.DataFrame):\n",
    "        # takeprofit = None\n",
    "        return self.detect_pattern(data, talib.CDLDRAGONFLYDOJI, \"dragonfly_doji\")\n",
    "\n",
    "    def gravestone_doji(self, data: pd.DataFrame):\n",
    "        # takeprofit = None\n",
    "        return self.detect_pattern(data, talib.CDLGRAVESTONEDOJI, \"gravestone_doji\")\n",
    "\n",
    "    def engulfing(self, data: pd.DataFrame):\n",
    "        return self.detect_pattern(data, talib.CDLENGULFING, \"engulfing\")\n",
    "\n",
    "    def morning_star(self, data: pd.DataFrame):\n",
    "        return self.detect_pattern(data, talib.CDLMORNINGSTAR, \"morning_star\")\n",
    "\n",
    "    def evening_star(self, data: pd.DataFrame):\n",
    "        return self.detect_pattern(data, talib.CDLEVENINGSTAR, \"evening_star\")\n",
    "\n",
    "    def morning_doji_star(self, data: pd.DataFrame):\n",
    "        # takeprofit = None\n",
    "        return self.detect_pattern(data, talib.CDLMORNINGDOJISTAR, \"morning_doji_star\")\n",
    "\n",
    "    def evening_doji_star(self, data: pd.DataFrame):\n",
    "        # takeprofit = None\n",
    "        return self.detect_pattern(data, talib.CDLEVENINGDOJISTAR, \"evening_doji_star\")\n",
    "\n",
    "    def hammer(self, data: pd.DataFrame):\n",
    "        return self.detect_pattern(data, talib.CDLHAMMER, \"hammer\")\n",
    "\n",
    "    def inverted_hammer(self, data: pd.DataFrame):\n",
    "        return self.detect_pattern(data, talib.CDLINVERTEDHAMMER, \"inverted_hammer\")\n",
    "\n",
    "    def hanging_man(self, data: pd.DataFrame):\n",
    "        return self.detect_pattern(data, talib.CDLHANGINGMAN, \"hanging_man\")\n",
    "\n",
    "    def shooting_star(self, data: pd.DataFrame):\n",
    "        return self.detect_pattern(data, talib.CDLSHOOTINGSTAR, \"shooting_star\")\n",
    "\n",
    "    def marubozu(self, data: pd.DataFrame):\n",
    "        # takeprofit = None\n",
    "        return self.detect_pattern(data, talib.CDLMARUBOZU, \"marubozu\")\n",
    "\n",
    "    def harami(self, data: pd.DataFrame):\n",
    "        # takeprofit = None\n",
    "        return self.detect_pattern(data, talib.CDLHARAMI, \"harami\")\n",
    "\n",
    "    def harami_cross(self, data: pd.DataFrame):\n",
    "        # takeprofit = None\n",
    "        return self.detect_pattern(data, talib.CDLHARAMICROSS, \"harami_cross\")\n",
    "\n",
    "    def spinning_top(self, data: pd.DataFrame):\n",
    "        '''\n",
    "        NEED FIBONACCI\n",
    "        '''\n",
    "        return self.detect_pattern(data, talib.CDLSPINNINGTOP, \"spinning_top\")\n",
    "\n",
    "    def kicking(self, data: pd.DataFrame):\n",
    "        # takeprofit = None\n",
    "        return self.detect_pattern(data, talib.CDLKICKING, \"kicking\")\n",
    "\n",
    "    def kicking_by_length(self, data: pd.DataFrame):\n",
    "        # takeprofit = None\n",
    "        return self.detect_pattern(data, talib.CDLKICKINGBYLENGTH, \"kicking_by_length\")\n",
    "\n",
    "    def tasuki_gap(self, data: pd.DataFrame):\n",
    "        # takeprofit = None\n",
    "        return self.detect_pattern(data, talib.CDLTASUKIGAP, \"tasuki_gap\")\n",
    "\n",
    "    def gap_side_by_side_white(self, data: pd.DataFrame):\n",
    "        # takeprofit = None\n",
    "        return self.detect_pattern(data, talib.CDLGAPSIDESIDEWHITE, \"gap_side_by_side_white\")\n",
    "\n",
    "    def counter_attack(self, data: pd.DataFrame):\n",
    "        # takeprofit = None\n",
    "        return self.detect_pattern(data, talib.CDLCOUNTERATTACK, \"counter_attack\")\n",
    "\n",
    "    def piercing(self, data: pd.DataFrame):\n",
    "        # takeprofit = None\n",
    "        return self.detect_pattern(data, talib.CDLPIERCING, \"piercing\")\n",
    "\n",
    "    def dark_cloud_cover(self, data: pd.DataFrame):\n",
    "        # takeprofit = None\n",
    "        return self.detect_pattern(data, talib.CDLDARKCLOUDCOVER, \"dark_cloud_cover\")\n",
    "\n",
    "    def tri_star(self, data: pd.DataFrame):\n",
    "        # takeprofit = None\n",
    "        return self.detect_pattern(data, talib.CDLTRISTAR, \"tri_star\")\n",
    "\n",
    "    def on_neck(self, data: pd.DataFrame):\n",
    "        # takeprofit = None\n",
    "        return self.detect_pattern(data, talib.CDLONNECK, \"on_neck\")\n",
    "\n",
    "    def in_neck(self, data: pd.DataFrame):\n",
    "        # takeprofit = None\n",
    "        return self.detect_pattern(data, talib.CDLINNECK, \"in_neck\")\n",
    "\n",
    "    def thrusting(self, data: pd.DataFrame):\n",
    "        # takeprofit = None\n",
    "        return self.detect_pattern(data, talib.CDLTHRUSTING, \"thrusting\")\n",
    "\n",
    "    def matching_low(self, data: pd.DataFrame):\n",
    "        # takeprofit = None\n",
    "        return self.detect_pattern(data, talib.CDLMATCHINGLOW, \"matching_low\")\n",
    "\n",
    "    def three_black_crows(self, data: pd.DataFrame):\n",
    "        # takeprofit = None\n",
    "        return self.detect_pattern(data, talib.CDL3BLACKCROWS, \"three_black_crows\")\n",
    "\n",
    "    def three_white_soldiers(self, data: pd.DataFrame):\n",
    "        # takeprofit = None\n",
    "        return self.detect_pattern(data, talib.CDL3WHITESOLDIERS, \"three_white_soldiers\")\n",
    "\n",
    "    def three_inside(self, data: pd.DataFrame):\n",
    "        # takeprofit = None\n",
    "        return self.detect_pattern(data, talib.CDL3INSIDE, \"three_inside\")\n",
    "\n",
    "    def three_outside(self, data: pd.DataFrame):\n",
    "        # takeprofit = None\n",
    "        return self.detect_pattern(data, talib.CDL3OUTSIDE, \"three_outside\")\n",
    "\n",
    "    def three_stars_in_south(self, data: pd.DataFrame):\n",
    "        # takeprofit = None\n",
    "        return self.detect_pattern(data, talib.CDL3STARSINSOUTH, \"three_stars_in_south\")\n",
    "\n",
    "    def advance_block(self, data: pd.DataFrame):\n",
    "        # takeprofit = None\n",
    "        return self.detect_pattern(data, talib.CDLADVANCEBLOCK, \"advance_block\")\n",
    "\n",
    "    def stalled_pattern(self, data: pd.DataFrame):\n",
    "        # takeprofit = None\n",
    "        return self.detect_pattern(data, talib.CDLSTALLEDPATTERN, \"stalled_pattern\")\n",
    "\n",
    "    # def abandoned_baby(self, data: pd.DataFrame):\n",
    "    #     return self.detect_pattern(data, talib.CDLABANDONEDBABY, \"abandoned_baby\")\n",
    "\n",
    "    # def unique_3_river(self, data: pd.DataFrame):\n",
    "    #     return self.detect_pattern(data, talib.CDLUNIQUE3RIVER, \"unique_3_river\")\n",
    "\n",
    "    # def belt_hold(self, data: pd.DataFrame):\n",
    "    #     return self.detect_pattern(data, talib.CDLBELTHOLD, \"belt_hold\")\n",
    "\n",
    "    # def separating_lines(self, data: pd.DataFrame):\n",
    "    #     return self.detect_pattern(data, talib.CDLSEPARATINGLINES, \"Separating Lines\")\n",
    "\n",
    "    # def upside_gap_two_crows(self, data: pd.DataFrame):\n",
    "    #     return self.detect_pattern(data, talib.CDLUPSIDEGAP2CROWS, \"Upside Gap Two Crows\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "symbol = 'SPY'\n",
    "start, end = '2015-01-01', '2024-07-05'\n",
    "\n",
    "data = yf.Ticker(symbol).history(period='1y', interval='1d', start=start, end=end)\n",
    "\n",
    "cm = CandlesPatterns()\n",
    "\n",
    "# Loop para detectar padrões\n",
    "for candle_function in dir(cm):\n",
    "    if (not candle_function.startswith(\"__\") and \n",
    "        callable(getattr(cm, candle_function)) and \n",
    "        candle_function != \"detect_pattern\"):\n",
    "        pattern_function = getattr(cm, candle_function)\n",
    "        try:\n",
    "            candle_result = pattern_function(data)\n",
    "        except Exception as e:\n",
    "            print(f\"Error detecting pattern {candle_function}: {e}\")\n",
    "\n",
    "# Verificar padrões detectados e valores de stoploss\n",
    "print(\"\\nAll Detected Patterns with Stoploss:\")\n",
    "cm.result_candles_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import talib\n",
    "import pandas as pd\n",
    "\n",
    "class TrendMetrics():\n",
    "    \"\"\"\n",
    "    A class that encapsulates technical analysis metrics using TA-Lib.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.result_df = pd.DataFrame(columns=['function', 'signal'])\n",
    "        self.crossover_info = pd.DataFrame(columns=['function', 'signal', 'period_low', 'period_mid', 'period_high', 'ema_low', 'ema_mid', 'ema_high'])\n",
    "        self.sma_bands_info = pd.DataFrame(columns=['function', 'signal', 'period', 'std', 'lower_band', 'middle_band', 'upper_band'])\n",
    "        self.rsi_info = pd.DataFrame(columns=['function', 'signal', 'period', 'upper_level', 'lower_level'])\n",
    "\n",
    "    def get_crossover(self, data: pd.DataFrame, l1: int, l2: int, l3: int):\n",
    "        \"\"\"\n",
    "        This function measures the crossover of 3 EMAs using TA-Lib.\n",
    "        \n",
    "        Parameters:\n",
    "        - data: DataFrame containing the price data with a 'close' column.\n",
    "        - l1, l2, l3: Periods for the 3 EMAs.\n",
    "        \n",
    "        Returns:\n",
    "        - Updates self.crossover_signal with 'Buy', 'Sell', or 'Flat'.\n",
    "        \"\"\"\n",
    "        period_low, period_mid, period_high = l1, l2, l3\n",
    "        \n",
    "        # Compute EMAs\n",
    "        ema1 = talib.EMA(data['Close'], timeperiod=l1)\n",
    "        ema2 = talib.EMA(data['Close'], timeperiod=l2)\n",
    "        ema3 = talib.EMA(data['Close'], timeperiod=l3)\n",
    "\n",
    "        # Current values\n",
    "        ema_low, ema_mid, ema_high = ema1.iloc[-1], ema2.iloc[-1], ema3.iloc[-1]\n",
    "\n",
    "        # Determine signal\n",
    "        if ema_low > ema_mid > ema_high:\n",
    "            crossover_signal = 'Buy'\n",
    "        elif ema_low < ema_mid < ema_high:\n",
    "            crossover_signal = 'Sell'\n",
    "        else:\n",
    "            crossover_signal = 'Flat'\n",
    "\n",
    "        # Save result\n",
    "        self.result_df = pd.concat([self.result_df, pd.DataFrame({\n",
    "            'function': ['Crossover'],\n",
    "            'signal': [crossover_signal]\n",
    "        })], ignore_index=True)\n",
    "\n",
    "        # Save result\n",
    "        self.crossover_info = pd.concat([self.crossover_info, pd.DataFrame({\n",
    "            'function': ['Crossover'],\n",
    "            'signal': [crossover_signal],\n",
    "            'period_low': [period_low],\n",
    "            'period_mid': [period_mid],\n",
    "            'period_high': [period_high],\n",
    "            'ema1_now': [ema_low],\n",
    "            'ema2_now': [ema_mid],\n",
    "            'ema3_now': [ema_high],\n",
    "        })], ignore_index=True)\n",
    "\n",
    "    def get_sma_bands(self, data: pd.DataFrame, length: int=15, std_dev: int = 1):\n",
    "        \"\"\"\n",
    "        This function calculates Bollinger Bands and detects signals based on them.\n",
    "        \n",
    "        Parameters:\n",
    "        - data: DataFrame containing the price data with a 'close' column.\n",
    "        - length: SMA period.\n",
    "        - std_dev: Number of standard deviations for the bands.\n",
    "        \n",
    "        Returns:\n",
    "        - Updates self.bbands_signal with 'Buy', 'Sell', or 'Flat'.\n",
    "        \"\"\"\n",
    "\n",
    "        period, std = length, std_dev\n",
    "\n",
    "        # Compute Bollinger Bands\n",
    "        upper_band, middle_band, lower_band = talib.BBANDS(\n",
    "            data['Close'], timeperiod=length, nbdevup=std_dev, nbdevdn=std_dev, matype=0\n",
    "        )\n",
    "\n",
    "        # Current values\n",
    "        last_close = data['Close'].iloc[-1]\n",
    "        lower_band, middle_band, upper_band = lower_band.iloc[-1], middle_band.iloc[-1], upper_band.iloc[-1]\n",
    "\n",
    "        # Determine signal\n",
    "        if last_close <= lower_band:\n",
    "            bbands_signal = 'Buy'\n",
    "        elif last_close >= upper_band:\n",
    "            bbands_signal = 'Sell'\n",
    "        else:\n",
    "            bbands_signal = 'Flat'\n",
    "\n",
    "        # Save result\n",
    "        self.result_df = pd.concat([self.result_df, pd.DataFrame({\n",
    "            'function': ['Bollinger_Bands'],\n",
    "            'signal': [bbands_signal]\n",
    "        })], ignore_index=True)\n",
    "        \n",
    "        self.sma_bands_info = pd.concat([self.sma_bands_info, pd.DataFrame({\n",
    "            'function': ['Bollinger_Bands'],\n",
    "            'signal': [bbands_signal],\n",
    "            'period': [period],\n",
    "            'std': [std],\n",
    "            'lower_band': [lower_band],\n",
    "            'middle_band': [middle_band],\n",
    "            'upper_band': [upper_band]\n",
    "        })], ignore_index=True)\n",
    "\n",
    "    def get_rsi(self, data: pd.DataFrame, length: int = 25, overbought: int = 70, oversold: int = 30):\n",
    "        \"\"\"\n",
    "        This function calculates the RSI and generates a signal based on overbought/oversold levels.\n",
    "        \n",
    "        Parameters:\n",
    "        - data: DataFrame containing the price data with a 'close' column.\n",
    "        - length: RSI period.\n",
    "        - overbought: RSI overbought threshold.\n",
    "        - oversold: RSI oversold threshold.\n",
    "        \n",
    "        Returns:\n",
    "        - Updates self.rsi_signal with 'Buy', 'Sell', or 'Flat'.\n",
    "        \"\"\"\n",
    "\n",
    "        period, upper_level, lower_level = length, overbought, oversold \n",
    "\n",
    "        # Compute RSI\n",
    "        rsi = talib.RSI(data['Close'], timeperiod=length)\n",
    "        rsi_now = rsi.iloc[-1]\n",
    "\n",
    "        # Determine signal\n",
    "        if rsi_now >= overbought:\n",
    "            rsi_signal = 'Sell'\n",
    "        elif rsi_now <= oversold:\n",
    "            rsi_signal = 'Buy'\n",
    "        else:\n",
    "            rsi_signal = 'Flat'\n",
    "\n",
    "        # Save result\n",
    "        self.result_df = pd.concat([self.result_df, pd.DataFrame({\n",
    "            'function': ['RSI'],\n",
    "            'signal': [rsi_signal]\n",
    "        })], ignore_index=True)\n",
    "\n",
    "        self.rsi_info = pd.concat([self.rsi_info, pd.DataFrame({\n",
    "            'function': ['RSI'],\n",
    "            'signal': [rsi_signal],\n",
    "            'period': [period],\n",
    "            'upper_level': [upper_level],\n",
    "            'lower_level': [lower_level]\n",
    "        })], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import talib\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "class ParamsOptimization():\n",
    "    \"\"\"\n",
    "    A class for optimizing technical indicators' parameters and evaluating strategy performance.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.crossover_params = pd.DataFrame(columns=['Ticker', 'EMA1', 'EMA2', 'EMA3', 'Sharpe', 'MaxDrawdown', 'Expectancy'])\n",
    "        self.bbands_params = pd.DataFrame(columns=['Ticker', 'Period', 'Std', 'Sharpe', 'MaxDrawdown', 'Expectancy'])\n",
    "\n",
    "    def optimize(self, asset_type: str, symbol: str, period: str, interval: str):\n",
    "        \"\"\"\n",
    "        Run optimization for all strategies.\n",
    "        \"\"\"\n",
    "        data = self.fetch_data(asset_type, symbol, period, interval)\n",
    "        self.crossover_results = self.optimize_crossover(data, symbol)\n",
    "        self.bbands_results = self.optimize_bbands(data, symbol)\n",
    "\n",
    "\n",
    "    def fetch_data(self,asset_type: str, symbol : str, period : str, interval : str):\n",
    "        \"\"\"\n",
    "        Simulate fetching market data for the given ticker. According to same periodicity and timeframe of subject bot setting\n",
    "        \"\"\"\n",
    "\n",
    "        if asset_type == 'stock':\n",
    "            from backend.datasources.yahoodata import DataHistory\n",
    "            dh = DataHistory()\n",
    "            data = dh.get_yahoo_data_history(symbol, period, interval, start=datetime.now(), end=datetime.now() - timedelta(days=365))\n",
    "\n",
    "        elif asset_type == 'cambial':\n",
    "            pass\n",
    "            # Metatrader\n",
    "        elif asset_type == 'crypto':\n",
    "            pass\n",
    "            # crypto\n",
    "        return data\n",
    "\n",
    "    def optimize_crossover(self, data : pd.DataFrame, symbol : str):\n",
    "        \"\"\"\n",
    "        Optimize EMA crossover strategy.\n",
    "        \"\"\"\n",
    "        ema1_periods = range(10, 21)\n",
    "        ema2_periods = range(25, 61)\n",
    "        ema3_periods = range(100, 200)\n",
    "\n",
    "        combinations = list(itertools.product(ema1_periods, ema2_periods, ema3_periods))\n",
    "\n",
    "        results = Parallel(n_jobs=-1)(delayed(self.simulate_crossover)(\n",
    "            data, symbol, l1, l2, l3) for l1, l2, l3 in tqdm(combinations, desc=\"Optimizing EMA Crossover\"))\n",
    "\n",
    "        results_df = pd.DataFrame(results)\n",
    "\n",
    "        return results_df\n",
    "\n",
    "    def simulate_crossover(self, data : pd.DataFrame, symbol : str, l1 : int, l2 : int, l3 : int):\n",
    "        \"\"\"\n",
    "        Simulate crossover strategy and calculate metrics.\n",
    "        \"\"\"\n",
    "        # Calculate EMAs\n",
    "        data['ema1'] = talib.EMA(data['Close'], timeperiod=l1)\n",
    "        data['ema2'] = talib.EMA(data['Close'], timeperiod=l2)\n",
    "        data['ema3'] = talib.EMA(data['Close'], timeperiod=l3)\n",
    "\n",
    "        # Generate signals\n",
    "        data['signal'] = np.where((data['ema1'] > data['ema2']) & (data['ema2'] > data['ema3']), 1,\n",
    "                                  np.where((data['ema1'] < data['ema2']) & (data['ema2'] < data['ema3']), -1, 0))\n",
    "        data['returns'] = data['Close'].pct_change() * data['signal'].shift(1)\n",
    "\n",
    "        # Calculate metrics\n",
    "        sharpe = self.calculate_sharpe(data['returns'])\n",
    "        max_drawdown = self.calculate_max_drawdown(data['returns'])\n",
    "        expectancy = self.calculate_expectancy(data['returns'])\n",
    "\n",
    "        return {\n",
    "            'Ticker': symbol,\n",
    "            'EMA1': l1,\n",
    "            'EMA2': l2,\n",
    "            'EMA3': l3,\n",
    "            'Sharpe': sharpe,\n",
    "            'MaxDrawdown': max_drawdown,\n",
    "            'Expectancy': expectancy\n",
    "        }\n",
    "\n",
    "    def optimize_bbands(self, data : pd.DataFrame, symbol : str):\n",
    "        \"\"\"\n",
    "        Optimize Bollinger Bands strategy.\n",
    "        \"\"\"\n",
    "        sma_periods = range(10, 21)\n",
    "        std_devs = range(1, 3)\n",
    "\n",
    "        combinations = list(itertools.product(sma_periods, std_devs))\n",
    "\n",
    "        results = Parallel(n_jobs=-1)(delayed(self.simulate_bbands)(\n",
    "            data, symbol, period, std) for period, std in tqdm(combinations, desc=\"Optimizing Bollinger Bands\"))\n",
    "\n",
    "        results_df = pd.DataFrame(results)\n",
    "       \n",
    "        return results_df\n",
    "\n",
    "    def simulate_bbands(self, data, symbol, period, std):\n",
    "        \"\"\"\n",
    "        Simulate Bollinger Bands strategy and calculate metrics.\n",
    "        \"\"\"\n",
    "        # Calculate Bollinger Bands\n",
    "        upperband, middleband, lowerband = talib.BBANDS(\n",
    "            data['Close'], timeperiod=period, nbdevup=std, nbdevdn=std, matype=0\n",
    "        )\n",
    "\n",
    "        # Generate signals\n",
    "        data['signal'] = np.where(data['Close'] < lowerband, 1,\n",
    "                                  np.where(data['Close'] > upperband, -1, 0))\n",
    "        data['returns'] = data['Close'].pct_change() * data['signal'].shift(1)\n",
    "\n",
    "        # Calculate metrics\n",
    "        sharpe = self.calculate_sharpe(data['returns'])\n",
    "        max_drawdown = self.calculate_max_drawdown(data['returns'])\n",
    "        expectancy = self.calculate_expectancy(data['returns'])\n",
    "\n",
    "        return {\n",
    "            'Ticker': symbol,\n",
    "            'Period': period,\n",
    "            'Std': std,\n",
    "            'Sharpe': sharpe,\n",
    "            'MaxDrawdown': max_drawdown,\n",
    "            'Expectancy': expectancy\n",
    "        }\n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_sharpe(returns, risk_free_rate=0.025):\n",
    "        \"\"\"\n",
    "        Calculate Sharpe Ratio.\n",
    "        \"\"\"\n",
    "        mean_return = returns.mean()\n",
    "        std_dev = returns.std()\n",
    "        if std_dev == 0:\n",
    "            return 0\n",
    "        return (mean_return - risk_free_rate) / std_dev\n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_max_drawdown(returns):\n",
    "        \"\"\"\n",
    "        Calculate Max Drawdown.\n",
    "        \"\"\"\n",
    "        cumulative = (1 + returns).cumprod()\n",
    "        running_max = cumulative.cummax()\n",
    "        drawdown = running_max - cumulative\n",
    "        return drawdown.max()\n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_expectancy(returns):\n",
    "        \"\"\"\n",
    "        Calculate Expectancy.\n",
    "        \"\"\"\n",
    "        wins = returns[returns > 0]\n",
    "        losses = returns[returns < 0]\n",
    "        win_rate = len(wins) / len(returns) if len(returns) > 0 else 0\n",
    "        loss_rate = 1 - win_rate\n",
    "        avg_win = wins.mean() if len(wins) > 0 else 0\n",
    "        avg_loss = losses.mean() if len(losses) > 0 else 0\n",
    "        return (win_rate * avg_win) - (loss_rate * avg_loss)\n",
    "\n",
    "# Adicionar Fontes Crypt e Cambial\n",
    "# Adicionar resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data\n",
    "\n",
    "tm = TrendMetrics()\n",
    "tm.get_sma_bands(data=df, length=15, std_dev=1)\n",
    "tm.get_crossover(data=df, l1=25, l2=50, l3=200)\n",
    "tm.get_rsi(data=df, length=25, overbought=70, oversold=30)\n",
    "\n",
    "tm.result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instanciar a classe\n",
    "optimizer = ParamsOptimization()\n",
    "\n",
    "# Testar otimização de EMA crossover\n",
    "crossover_results = optimizer.optimize_crossover(data, symbol=symbol)\n",
    "print(\"Resultados da Otimização EMA Crossover:\")\n",
    "print(crossover_results)\n",
    "\n",
    "# Testar otimização de Bollinger Bands\n",
    "bbands_results = optimizer.optimize_bbands(data, symbol=symbol)\n",
    "print(\"\\nResultados da Otimização Bollinger Bands:\")\n",
    "print(bbands_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Yahoo Tables scrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_yahoo_most_active(table_class: str = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Extrat data from yahoo most active\n",
    "\n",
    "    Parameters:\n",
    "        classe_tabela (str): Classe CSS da tabela para extração (opcional).\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with table content.\n",
    "    \"\"\"\n",
    "\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.get(\"https://finance.yahoo.com/markets/stocks/most-active/\", headers=headers)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        if table_class:\n",
    "            tabela = soup.find('table', {'class': table_class})\n",
    "        else:\n",
    "            tabela = soup.find('table')\n",
    "\n",
    "        headers = [th.text.strip() for th in tabela.find_all('th')]\n",
    "\n",
    "        rows = []\n",
    "        for row in tabela.find_all('tr')[1:]:\n",
    "            cols = [td.text.strip() for td in row.find_all('td')]\n",
    "            if cols: \n",
    "                rows.append(cols)\n",
    "\n",
    "        df = pd.DataFrame(rows, columns=headers if headers else None)\n",
    "        df[\"Price\"] = df[\"Price\"].str.extract(r\"^([\\d\\.]+)\")\n",
    "        df[\"Change\"] = df[\"Change\"].str.replace(\"+\", \"\", regex=False).astype(float)\n",
    "        df[\"Change %\"] = df[\"Change %\"].str.replace(\"%\", \"\", regex=False).astype(float)\n",
    "        df[\"P/E Ratio (TTM)\"] = df[\"P/E Ratio (TTM)\"].str.replace(\"-\", \"0\", regex=False).astype(float)\n",
    "        df[\"52 Wk Change %\"] = df[\"52 Wk Change %\"].str.replace(\"%\", \"\", regex=False).astype(float)\n",
    "        df.drop(columns=[\"52 Wk Range\"], inplace=True)\n",
    "        return df\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error accessing URL: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing data: {e}\")\n",
    "\n",
    "    return pd.DataFrame()\n",
    "\n",
    "df = get_yahoo_most_active()\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_yahoo_trending(table_class: str = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Extrat data from yahoo trending\n",
    "\n",
    "    Parameters:\n",
    "        classe_tabela (str): Classe CSS da tabela para extração (opcional).\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with table content.\n",
    "    \"\"\"\n",
    "\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.get(\"https://finance.yahoo.com/markets/stocks/trending/\", headers=headers)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        if table_class:\n",
    "            tabela = soup.find('table', {'class': table_class})\n",
    "        else:\n",
    "            tabela = soup.find('table')\n",
    "\n",
    "        headers = [th.text.strip() for th in tabela.find_all('th')]\n",
    "\n",
    "        rows = []\n",
    "        for row in tabela.find_all('tr')[1:]:\n",
    "            cols = [td.text.strip() for td in row.find_all('td')]\n",
    "            if cols: \n",
    "                rows.append(cols)\n",
    "\n",
    "        df = pd.DataFrame(rows, columns=headers if headers else None)\n",
    "        df[\"Price\"] = df[\"Price\"].str.extract(r\"^([\\d\\.]+)\")\n",
    "        df[\"Change\"] = df[\"Change\"].str.replace(\"+\", \"\", regex=False).astype(float)\n",
    "        df[\"Change %\"] = df[\"Change %\"].str.replace(\"%\", \"\", regex=False).astype(float)\n",
    "        df[\"Avg Vol (3M)\"] = df[\"Avg Vol (3M)\"].str.replace(\"-\", \"0\", regex=False)\n",
    "        df[\"Market Cap\"] = df[\"Market Cap\"].str.replace(\"-\", \"0\", regex=False)\n",
    "        df[\"P/E Ratio (TTM)\"] = df[\"P/E Ratio (TTM)\"].str.replace(\"-\", \"0\", regex=False).astype(float)\n",
    "        df[\"52 Wk Change %\"] = df[\"52 Wk Change %\"].str.replace(\"%\", \"\", regex=False).astype(float)\n",
    "        df.drop(columns=[\"52 Wk Range\"], inplace=True)\n",
    "        return df\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error accessing URL: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing data: {e}\")\n",
    "\n",
    "    return pd.DataFrame()\n",
    "\n",
    "df_trend = get_yahoo_trending()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_yahoo_top100_gainers(table_class: str = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Extrat data from yahoo top 100 gainers\n",
    "\n",
    "    Parameters:\n",
    "        classe_tabela (str): Classe CSS da tabela para extração (opcional).\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with table content.\n",
    "    \"\"\"\n",
    "\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.get(\"https://finance.yahoo.com/markets/stocks/gainers/?start=0&count=100\", headers=headers)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        if table_class:\n",
    "            tabela = soup.find('table', {'class': table_class})\n",
    "        else:\n",
    "            tabela = soup.find('table')\n",
    "\n",
    "        headers = [th.text.strip() for th in tabela.find_all('th')]\n",
    "\n",
    "        rows = []\n",
    "        for row in tabela.find_all('tr')[1:]:\n",
    "            cols = [td.text.strip() for td in row.find_all('td')]\n",
    "            if cols: \n",
    "                rows.append(cols)\n",
    "\n",
    "        df = pd.DataFrame(rows, columns=headers if headers else None)\n",
    "        df[\"Price\"] = df[\"Price\"].str.extract(r\"^([\\d\\.]+)\")\n",
    "        df[\"Change\"] = df[\"Change\"].str.replace(\"+\", \"\", regex=False)\n",
    "        df[\"Change %\"] = df[\"Change %\"].str.replace(\"%\", \"\", regex=False)\n",
    "        df[\"Volume\"] = df[\"Volume\"].str.replace(\"-\", \"0\", regex=False)\n",
    "        df[\"Avg Vol (3M)\"] = df[\"Avg Vol (3M)\"].str.replace(\"-\", \"0\", regex=False)\n",
    "        df[\"Market Cap\"] = df[\"Market Cap\"].str.replace(\"-\", \"0\", regex=False)\n",
    "        df[\"P/E Ratio (TTM)\"] = df[\"P/E Ratio (TTM)\"].str.replace(\"-\", \"0\", regex=False)\n",
    "        df[\"52 Wk Change %\"] = df[\"52 Wk Change %\"].str.replace(\"%\", \"\", regex=False)\n",
    "        df.drop(columns=[\"52 Wk Range\"], inplace=True)\n",
    "        return df\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error accessing URL: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing data: {e}\")\n",
    "\n",
    "    return pd.DataFrame()\n",
    "\n",
    "df_top_gainers = get_yahoo_top100_gainers()\n",
    "df_top_gainers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_yahoo_forex(table_class: str = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Extrat data from yahoo currencies.\n",
    "\n",
    "    Parameters:\n",
    "        classe_tabela (str): Class CSS from table to scrap (optional).\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with table content.\n",
    "    \"\"\"\n",
    "\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.get(\"https://finance.yahoo.com/markets/currencies/\", headers=headers)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        if table_class:\n",
    "            tabela = soup.find('table', {'class': table_class})\n",
    "        else:\n",
    "            tabela = soup.find('table')\n",
    "\n",
    "        headers = [th.text.strip() for th in tabela.find_all('th')]\n",
    "\n",
    "        rows = []\n",
    "        for row in tabela.find_all('tr')[1:]:\n",
    "            cols = [td.text.strip() for td in row.find_all('td')]\n",
    "            if cols:\n",
    "                rows.append(cols)\n",
    "\n",
    "        df = pd.DataFrame(rows, columns=headers if headers else None)\n",
    "        df[\"Price\"] = df[\"Price\"].str.extract(r\"^([\\d\\.]+)\")\n",
    "        df[\"Change\"] = df[\"Change\"].str.replace(\"+\", \"\", regex=False)\n",
    "        df[\"Change %\"] = df[\"Change %\"].str.replace(\"%\", \"\", regex=False).str.replace(\"+\", \"\", regex=False)\n",
    "        df.drop(columns=[\"52 Wk Range\", \"Symbol\"], inplace=True)\n",
    "        df.rename(columns={\"Name\": \"Symbol\"}, inplace=True)\n",
    "        \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error accessing URL: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing data: {e}\")\n",
    " \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_forex = get_yahoo_forex()\n",
    "df_forex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_yahoo_indices(table_class: str = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Extrat data from yahoo world indices.\n",
    "\n",
    "    Parameters:\n",
    "        classe_tabela (str): Class CSS from table to scrap (optional).\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with table content.\n",
    "    \"\"\"\n",
    "\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.get(\"https://finance.yahoo.com/markets/world-indices/\", headers=headers)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        if table_class:\n",
    "            tabela = soup.find('table', {'class': table_class})\n",
    "        else:\n",
    "            tabela = soup.find('table')\n",
    "\n",
    "        headers = [th.text.strip() for th in tabela.find_all('th')]\n",
    "\n",
    "        rows = []\n",
    "        for row in tabela.find_all('tr')[1:]:\n",
    "            cols = [td.text.strip() for td in row.find_all('td')]\n",
    "            if cols:\n",
    "                rows.append(cols)\n",
    "\n",
    "        df = pd.DataFrame(rows, columns=headers if headers else None)\n",
    "        df[\"Price\"] = df[\"Price\"].str.extract(r\"^([\\d\\.]+)\")\n",
    "        df[\"Change\"] = df[\"Change\"].str.replace(\"+\", \"\", regex=False)\n",
    "        df[\"Change %\"] = df[\"Change %\"].str.replace(\"%\", \"\", regex=False).str.replace(\"+\", \"\", regex=False)\n",
    "        df[\"Volume\"] = df[\"Volume\"].str.replace(\"-\", \"0\", regex=False)\n",
    "        df.drop(columns=[\"52 Wk Range\", \"Day Range\"], inplace=True)\n",
    "        \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error accessing URL: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing data: {e}\")\n",
    " \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_indices = get_yahoo_indices()\n",
    "df_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_yahoo_crypto_top100_gainers(table_class: str = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Extrat data from yahoo crypto top 100 gainers\n",
    "\n",
    "    Parameters:\n",
    "        classe_tabela (str): Class CSS from table to scrap (optional).\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with table content.\n",
    "    \"\"\"\n",
    "\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.get(\"https://finance.yahoo.com/markets/crypto/gainers/?start=0&count=100\", headers=headers)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        if table_class:\n",
    "            tabela = soup.find('table', {'class': table_class})\n",
    "        else:\n",
    "            tabela = soup.find('table')\n",
    "\n",
    "        headers = [th.text.strip() for th in tabela.find_all('th')]\n",
    "\n",
    "        rows = []\n",
    "        for row in tabela.find_all('tr')[1:]:\n",
    "            cols = [td.text.strip() for td in row.find_all('td')]\n",
    "            if cols: \n",
    "                rows.append(cols)\n",
    "\n",
    "        df = pd.DataFrame(rows, columns=headers if headers else None)\n",
    "        df[\"Price\"] = df[\"Price\"].str.extract(r\"^([\\d\\.]+)\")\n",
    "        df[\"Change\"] = df[\"Change\"].str.replace(\"+\", \"\", regex=False)\n",
    "        df[\"Change %\"] = df[\"Change %\"].str.replace(\"%\", \"\", regex=False).str.replace(\"+\", \"\", regex=False)\n",
    "        df[\"Volume\"] = df[\"Volume\"].str.replace(\"-\", \"0\", regex=False)\n",
    "        df[\"Volume In Currency (24hr)\"] = df[\"Volume In Currency (24hr)\"].str.replace(\"-\", \"0\", regex=False)\n",
    "        df[\"Total Volume All Currencies (24hr)\"] = df[\"Total Volume All Currencies (24hr)\"].str.replace(\"-\", \"0\", regex=False)\n",
    "        df[\"52 Wk Change %\"] = df[\"52 Wk Change %\"].str.replace(\"%\", \"\", regex=False)\n",
    "        df.drop(columns=[\"52 Wk Range\"], inplace=True)\n",
    "        return df\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error accessing URL: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing data: {e}\")\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_crypto = get_yahoo_crypto_top100_gainers()\n",
    "df_crypto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_yahoo_crypto_top100_most_active(table_class: str = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Extrat data from yahoo crypto top 100 msot active.\n",
    "\n",
    "    Parameters:\n",
    "        classe_tabela (str): Class CSS from table to scrap (optional).\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with table content.\n",
    "    \"\"\"\n",
    "\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.get(\"https://finance.yahoo.com/markets/crypto/most-active/?start=0&count=100\", headers=headers)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        if table_class:\n",
    "            tabela = soup.find('table', {'class': table_class})\n",
    "        else:\n",
    "            tabela = soup.find('table')\n",
    "\n",
    "        headers = [th.text.strip() for th in tabela.find_all('th')]\n",
    "\n",
    "        rows = []\n",
    "        for row in tabela.find_all('tr')[1:]:\n",
    "            cols = [td.text.strip() for td in row.find_all('td')]\n",
    "            if cols: \n",
    "                rows.append(cols)\n",
    "\n",
    "        df = pd.DataFrame(rows, columns=headers if headers else None)\n",
    "        df[\"Price\"] = df[\"Price\"].str.extract(r\"^([\\d\\.]+)\")\n",
    "        df[\"Change\"] = df[\"Change\"].str.replace(\"+\", \"\", regex=False)\n",
    "        df[\"Change %\"] = df[\"Change %\"].str.replace(\"%\", \"\", regex=False).str.replace(\"+\", \"\", regex=False)\n",
    "        df[\"Volume\"] = df[\"Volume\"].str.replace(\"-\", \"0\", regex=False)\n",
    "        df[\"Volume In Currency (24hr)\"] = df[\"Volume In Currency (24hr)\"].str.replace(\"-\", \"0\", regex=False)\n",
    "        df[\"Total Volume All Currencies (24hr)\"] = df[\"Total Volume All Currencies (24hr)\"].str.replace(\"-\", \"0\", regex=False)\n",
    "        df[\"52 Wk Change %\"] = df[\"52 Wk Change %\"].str.replace(\"%\", \"\", regex=False)\n",
    "        df.drop(columns=[\"52 Wk Range\"], inplace=True)\n",
    "        return df\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error accessing URL: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing data: {e}\")\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_crypto_most = get_yahoo_crypto_top100_most_active()\n",
    "df_crypto_most"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_yahoo_crypto_trending(table_class: str = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Extrat data from yahoo trending cryptos.\n",
    "\n",
    "    Parameters:\n",
    "        classe_tabela (str): Class CSS from table to scrap (optional).\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with table content.\n",
    "    \"\"\"\n",
    "\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.get(\"https://finance.yahoo.com/markets/crypto/trending/\", headers=headers)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        if table_class:\n",
    "            tabela = soup.find('table', {'class': table_class})\n",
    "        else:\n",
    "            tabela = soup.find('table')\n",
    "\n",
    "        headers = [th.text.strip() for th in tabela.find_all('th')]\n",
    "\n",
    "        rows = []\n",
    "        for row in tabela.find_all('tr')[1:]:\n",
    "            cols = [td.text.strip() for td in row.find_all('td')]\n",
    "            if cols: \n",
    "                rows.append(cols)\n",
    "\n",
    "        df = pd.DataFrame(rows, columns=headers if headers else None)\n",
    "        df[\"Price\"] = df[\"Price\"].str.extract(r\"^([\\d\\.]+)\")\n",
    "        df[\"Change\"] = df[\"Change\"].str.replace(\"+\", \"\", regex=False)\n",
    "        df[\"Change %\"] = df[\"Change %\"].str.replace(\"%\", \"\", regex=False).str.replace(\"+\", \"\", regex=False)\n",
    "        df[\"Market Cap\"] = df[\"Market Cap\"].str.replace(\"-\", \"0\", regex=False)\n",
    "        df[\"Volume\"] = df[\"Volume\"].str.replace(\"-\", \"0\", regex=False)\n",
    "        df[\"Volume In Currency (24hr)\"] = df[\"Volume In Currency (24hr)\"].str.replace(\"-\", \"0\", regex=False)\n",
    "        df[\"Total Volume All Currencies (24hr)\"] = df[\"Total Volume All Currencies (24hr)\"].str.replace(\"-\", \"0\", regex=False)\n",
    "        df[\"52 Wk Change %\"] = df[\"52 Wk Change %\"].str.replace(\"%\", \"\", regex=False).str.replace(\"-\", \"0\", regex=False)\n",
    "        df.drop(columns=[\"52 Wk Range\"], inplace=True)\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error accessing URL: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing data: {e}\")\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trend_cryp = get_yahoo_crypto_trending()\n",
    "df_trend_cryp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Yahoo Fundamental"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_yahoo_economic_calendar(table_class: str = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Extrat data from trading economics callendar.\n",
    "\n",
    "    Parameters:\n",
    "        classe_tabela (str): Class CSS from table to scrap (optional).\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with table content.\n",
    "    \"\"\"\n",
    "\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.get(\"https://tradingeconomics.com/calendar\", headers=headers)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        if table_class:\n",
    "            tabela = soup.find('table', {'class': table_class})\n",
    "        else:\n",
    "            tabela = soup.find('table')\n",
    "        \n",
    "        headers = [th.text.strip() for th in tabela.find_all('th')]\n",
    "        print(headers)\n",
    "        rows = []\n",
    "        for row in tabela.find_all('tr')[1:]:\n",
    "            cols = [td.text.strip() for td in row.find_all('td')]\n",
    "            if cols: \n",
    "                rows.append(cols)\n",
    "\n",
    "        df = pd.DataFrame(rows, columns=headers if headers else None)\n",
    "        # df[\"Price\"] = df[\"Price\"].str.extract(r\"^([\\d\\.]+)\")\n",
    "        # df[\"Change\"] = df[\"Change\"].str.replace(\"+\", \"\", regex=False)\n",
    "        # df[\"Change %\"] = df[\"Change %\"].str.replace(\"%\", \"\", regex=False).str.replace(\"+\", \"\", regex=False)\n",
    "        # df[\"Market Cap\"] = df[\"Market Cap\"].str.replace(\"-\", \"0\", regex=False)\n",
    "        # df[\"Volume\"] = df[\"Volume\"].str.replace(\"-\", \"0\", regex=False)\n",
    "        # df[\"Volume In Currency (24hr)\"] = df[\"Volume In Currency (24hr)\"].str.replace(\"-\", \"0\", regex=False)\n",
    "        # df[\"Total Volume All Currencies (24hr)\"] = df[\"Total Volume All Currencies (24hr)\"].str.replace(\"-\", \"0\", regex=False)\n",
    "        # df[\"52 Wk Change %\"] = df[\"52 Wk Change %\"].str.replace(\"%\", \"\", regex=False).str.replace(\"-\", \"0\", regex=False)\n",
    "        # df.drop(columns=[\"52 Wk Range\"], inplace=True)\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error accessing URL: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing data: {e}\")\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "}\n",
    "table_class = None\n",
    "try:\n",
    "    response = requests.get(\"https://tradingeconomics.com/calendar\", headers=headers)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    if table_class:\n",
    "        tabela = soup.find('table', {'class': table_class})\n",
    "    else:\n",
    "        tabela = soup.find('table')\n",
    "    \n",
    "    headers = [th.text.strip() for th in tabela.find_all('th')]\n",
    "    print(headers)\n",
    "\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f\"Error accessing URL: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error processing data: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_calendar = get_yahoo_economic_calendar()\n",
    "df_calendar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collection Binance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = \"https://api.binance.com/api/v3/ticker/price\"\n",
    "params = {\"symbol\": \"BTCUSDT\"}\n",
    "\n",
    "response = requests.get(url, params=params)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    symbol = response.json()[\"symbol\"]\n",
    "    price = response.json()[\"price\"]\n",
    "else:\n",
    "    error = response.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from decimal import Decimal\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "def get_crypto_symbol_24h(symbol : str):\n",
    "\n",
    "    url = \"https://api.binance.com/api/v3/ticker/24hr\"\n",
    "    params = {\"symbol\": symbol}\n",
    "\n",
    "    response = requests.get(url, params=params)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        priceChangePercent = Decimal(response.json()[\"priceChangePercent\"])\n",
    "        weightedAvgPrice = Decimal(response.json()[\"weightedAvgPrice\"])\n",
    "        prevClosePrice = Decimal(response.json()[\"prevClosePrice\"])\n",
    "        priceChange = Decimal(response.json()[\"priceChange\"])\n",
    "        lastPrice = Decimal(response.json()[\"lastPrice\"])\n",
    "        lastQty = Decimal(response.json()[\"lastQty\"])\n",
    "        bidPrice = Decimal(response.json()[\"bidPrice\"])\n",
    "        bidQty = Decimal(response.json()[\"bidQty\"])\n",
    "        askPrice = Decimal(response.json()[\"askPrice\"])\n",
    "        askQty = Decimal(response.json()[\"askQty\"])\n",
    "        openPrice = Decimal(response.json()[\"openPrice\"])\n",
    "        highPrice = Decimal(response.json()[\"highPrice\"])\n",
    "        lowPrice = Decimal(response.json()[\"lowPrice\"])\n",
    "        volume = Decimal(response.json()[\"volume\"])\n",
    "        quoteVolume = Decimal(response.json()[\"quoteVolume\"])\n",
    "        openTime = datetime.fromtimestamp(response.json()[\"openTime\"] / 1000).strftime(\"%d-%m-%Y %H:%M:%S\")\n",
    "        closeTime = datetime.fromtimestamp(response.json()[\"closeTime\"] / 1000).strftime(\"%d-%m-%Y %H:%M:%S\")\n",
    "        firstId = response.json()[\"firstId\"]\n",
    "        lastId = response.json()[\"lastId\"]\n",
    "        count = response.json()[\"count\"]\n",
    "    else:\n",
    "        print(f\"Erro: {response.status_code} - {response.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://api.binance.com/api/v3/openOrders\"\n",
    "params = {\"symbol\": \"BTCUSDT\"}\n",
    "\n",
    "response = requests.get(url, params=params)\n",
    "response.json()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
